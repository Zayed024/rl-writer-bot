
import json
import os
import random

PROMPTS_FILE = 'prompt_scores.json'

DEFAULT_PROMPTS = {
    "descriptive_evocative": {
        "template": (
            "Rewrite the following text, making it more vivid, descriptive, and emotionally resonant. "
            "Focus on sensory details and evocative language. Maintain the core narrative.\n\n"
        ),
        "score": 1.0 # Initial score
    },
    "concise_direct": {
        "template": (
            "Condense and clarify the following text, making it more concise and direct. "
            "Remove unnecessary words while preserving all essential information.\n\n"
        ),
        "score": 1.0
    },
    "formal_academic": {
        "template": (
            "Rewrite the following text into a formal and academic style. "
            "Use precise terminology and a neutral, objective tone. Avoid colloquialisms.\n\n"
        ),
        "score": 1.0
    }
    # can add more default prompt templates here 
    # New prompts generated by the AI will be added to this file dynamically.
}

def load_prompt_scores():
    
    if os.path.exists(PROMPTS_FILE):
        try:
            with open(PROMPTS_FILE, 'r', encoding='utf-8') as f:
                scores = json.load(f)
                #new default prompts are added if file exists but lacks them
                for name, default_data in DEFAULT_PROMPTS.items():
                    if name not in scores:
                        scores[name] = default_data
                return scores
        except json.JSONDecodeError:
            print(f"Warning: {PROMPTS_FILE} is corrupted. Initializing with default prompts.")
    return DEFAULT_PROMPTS.copy()

def save_prompt_scores(scores):
    
    with open(PROMPTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(scores, f, indent=4)
    print(f"Prompt scores saved to {PROMPTS_FILE}")

def get_adaptive_prompt(current_scores: dict, exploration_rate: float = 0.35):
    """
    Selects a prompt template adaptively based on scores, with an exploration rate (Epsilon-Greedy).
    Prompts with very low scores (< -5.0) are temporarily excluded from selection.
    """
    
    prompt_names = [name for name, data in current_scores.items() if data['score'] > -5.0]
    
    if not prompt_names: # Fallback if all prompts are too low or no prompts exist
        print("  [Prompt Manager] All prompts have very low scores or no prompts. Resetting to default.")
        current_scores.clear() 
        current_scores.update(DEFAULT_PROMPTS.copy()) 
        save_prompt_scores(current_scores) 
        prompt_names = list(current_scores.keys())
        if not prompt_names: # This condition should ideally not be met if DEFAULT_PROMPTS is populated
            return None, None

    if random.random() < exploration_rate:
        # Explore: choose a random prompt from the current valid pool
        chosen_name = random.choice(prompt_names)
        print(f"  [Prompt Manager] Exploring: Randomly chose '{chosen_name}' (Exploration Rate: {exploration_rate})")
    else:
       
        chosen_name = max(prompt_names, key=lambda name: current_scores[name]["score"])
        print(f"  [Prompt Manager]: Chose '{chosen_name}' (Score: {current_scores[chosen_name]['score']:.2f})")

    return chosen_name, current_scores[chosen_name]["template"]

def update_prompt_score(prompt_name: str, reward: float, current_scores: dict, learning_rate: float = 0.1):
    """
    Updates the score of a specific prompt based on the received reward.
    Uses a simple weighted average update.
    """
    if prompt_name in current_scores:
        current_scores[prompt_name]["score"] += reward * learning_rate
        # Keep scores bounded to prevent runaway values
        current_scores[prompt_name]["score"] = max(-10.0, min(10.0, current_scores[prompt_name]["score"])) 
        print(f"  [Prompt Manager] Updated score for '{prompt_name}': {current_scores[prompt_name]['score']:.2f} (Reward: {reward:.2f})")
    else:
        print(f"  [Prompt Manager] Warning: Attempted to update score for unknown prompt '{prompt_name}'.")

def add_new_prompt_template(name: str, template: str, current_scores: dict, initial_score: float = 0.0):
    """
    Adds a new prompt template to the collection if it doesn't already exist.
    Assigns a neutral initial score, as it hasn't been evaluated yet.
    """
    if name not in current_scores:
        current_scores[name] = {"template": template, "score": initial_score}
        print(f"  [Prompt Manager] Added new prompt template: '{name}' with initial score {initial_score}.")
    else:
        print(f"  [Prompt Manager] Prompt template '{name}' already exists. Skipping addition.")